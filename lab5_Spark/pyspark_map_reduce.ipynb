{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK7f3i7mLeMA",
        "outputId": "051e2d26-2bca-4cf8-9548-bcc671940c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=50b69d962dcf5774b407ef1d6aaedfad6b3b885eeef387289d1f74e570a8db8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import expr, hour, count, max, col, length\n",
        "from functools import reduce"
      ],
      "metadata": {
        "id": "2ZL3cB1EL0Bs"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver_feedback_categories_good = [\n",
        "    \"great service\",\n",
        "    \"nice car\",\n",
        "    \"wonderful companion\",\n",
        "    \"neat and tidy\",\n",
        "    \"expert navigation\",\n",
        "    \"recommend\",\n",
        "]\n",
        "driver_feedback_categories_bad = [\n",
        "    \"awful service\",\n",
        "    \"bad car\",\n",
        "    \"unpleasant companion\",\n",
        "    \"dirty\",\n",
        "    \"non-expert navigation\",\n",
        "    \"not recommend\",\n",
        "]"
      ],
      "metadata": {
        "id": "6lCNSLnFNHuG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_drivers(df: DataFrame, k: int):\n",
        "    return (\n",
        "        df.where(df.driver_rate.isNotNull())\n",
        "        .orderBy(df.driver_rate, ascending=False)\n",
        "        .select(\"driver_id\", \"driver_rate\")\n",
        "        .limit(k)\n",
        "        .rdd.map(lambda row: {\"driver_id\": row[0], \"driver_rate\": row[1]})\n",
        "    )\n",
        "\n",
        "\n",
        "def top_k_clients(df: DataFrame, k: int):\n",
        "    return (\n",
        "        df.where(df.client_rate.isNotNull())\n",
        "        .orderBy(df.client_rate, ascending=False)\n",
        "        .select(\"client_id\", \"client_rate\")\n",
        "        .limit(100)\n",
        "        .rdd.map(lambda row: {\"client_id\": row[0], \"client_rate\": row[1]})\n",
        "    )\n",
        "\n",
        "\n",
        "def top_k_drivers_by_profit(df: DataFrame, k: int):\n",
        "    return (\n",
        "        df.withColumn('cost', df.cost.cast('int'))\n",
        "        .groupBy('driver_id')\n",
        "        .agg(sum(df.cost).alias(\"profit\"))\n",
        "        .orderBy(\"profit\", ascending=False)\n",
        "        .select(\"driver_id\", \"profit\")\n",
        "        .limit(k)\n",
        "        .rdd.map(lambda row: {\"driver_id\": row[0], \"profit\": row[1]})\n",
        "    )\n",
        "\n",
        "\n",
        "def worst_drivers(df: DataFrame):\n",
        "    return (\n",
        "        df.where(df.driver_rate < 3.5)\n",
        "        .orderBy(df.driver_rate, ascending=True)\n",
        "        .select(\"driver_id\", \"driver_rate\")\n",
        "        .limit(100)\n",
        "        .rdd.map(lambda row: {\"driver_id\": row[0], \"driver_rate\": row[1]})\n",
        "    )\n",
        "\n",
        "\n",
        "def top_10_longest_text_comment(df: DataFrame):\n",
        "    return (\n",
        "        df.orderBy(length(df.text_driver_feedback), ascending=False)\n",
        "        .select(\"driver_id\", \"driver_rate\", \"text_driver_feedback\")\n",
        "        .limit(10)\n",
        "        .rdd.map(\n",
        "            lambda row: {\n",
        "                \"driver_id\": row[0],\n",
        "                \"driver_rate\": row[1],\n",
        "                \"text_driver_feedback\": row[2],\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def top_driver_feedback_category(df: DataFrame):\n",
        "    return (\n",
        "        df.where(reduce(lambda a, b: a|b, (df.category_driver_feedback.like(\"%\"+category+\"%\") for category in driver_feedback_categories_good)))\n",
        "        .groupBy('category_driver_feedback')\n",
        "        .count()\n",
        "        .orderBy('count', ascending=False)\n",
        "        .select('category_driver_feedback')\n",
        "        .limit(1)\n",
        "        .rdd.map( lambda row: {'category_driver_feedback': row[0]})\n",
        "    )\n",
        "\n",
        "\n",
        "def top_complaint_feedback_category(df: DataFrame):\n",
        "  return (\n",
        "        df.where(reduce(lambda a, b: a|b, (df.category_driver_feedback.like(\"%\"+category+\"%\") for category in driver_feedback_categories_bad)))\n",
        "        .groupBy('category_driver_feedback')\n",
        "        .count()\n",
        "        .orderBy('count', ascending=False)\n",
        "        .select('category_driver_feedback')\n",
        "        .limit(1)\n",
        "        .rdd.map( lambda row: {'category_driver_feedback': row[0]})\n",
        "    )\n",
        "\n",
        "\n",
        "def top_night_riders(df: DataFrame, k: int):\n",
        "    df = df.withColumn(\"hour\", hour(df.start_time))\n",
        "    df = df.withColumn(\n",
        "        \"daytime\", expr(\"case when hour > 0 and hour < 7 then 'night' else 'day' end\")\n",
        "    )\n",
        "    windowSpec_hour = Window.partitionBy(\"driver_id\", \"daytime\")\n",
        "    windowSpec = Window.partitionBy(\"driver_id\")\n",
        "    df_1 = df.withColumn(\"driver_rides\", count(df.client_id).over(windowSpec))\n",
        "    df_1 = df_1.withColumn(\"hour_rides\", count(df.client_id).over(windowSpec_hour))\n",
        "    df_1 = df_1.withColumn(\"pct_rides\", df_1.hour_rides / df_1.driver_rides)\n",
        "    return (\n",
        "        df_1.where(df_1.daytime == \"night\")\n",
        "        .orderBy(\"pct_rides\", ascending=False)\n",
        "        .dropDuplicates([\"driver_id\", \"hour_rides\"])\n",
        "        .select(\"driver_id\", \"hour_rides\", \"pct_rides\")\n",
        "        .limit(k)\n",
        "        .rdd.map(\n",
        "            lambda row: {\n",
        "                \"driver_id\": row[0],\n",
        "                \"night_rides\": row[1],\n",
        "                \"pct_rides\": row[2],\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def densest_traffic_by_hour(df: DataFrame):\n",
        "    df = df.withColumn(\"hour\", hour(df.start_time))\n",
        "    return (\n",
        "        df.groupBy(df.hour)\n",
        "        .agg(count(df.driver_id).alias(\"count_rides\"))\n",
        "        .orderBy(\"count_rides\", ascending=False)\n",
        "        .select(\"hour\", \"count_rides\")\n",
        "        .limit(1)\n",
        "        .rdd.map(\n",
        "            lambda row: {\"hour\": f\"{row[0]}-{(row[0]+1)//24}\", \"count_rides\": row[1]}\n",
        "        )\n",
        "    )\n"
      ],
      "metadata": {
        "id": "t9JIibPdhEvU"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    spark = (\n",
        "        SparkSession.builder.master(\"local\")\n",
        "        .appName(\"SparkDemo\")\n",
        "        .config(\"\")\n",
        "        .getOrCreate()\n",
        "    )\n",
        "    df = spark.read.csv(\"/content/drive/MyDrive/rides.csv\", header=True)\n",
        "    # top-100 drivers - variant 1 success\n",
        "    with open(\"data/top_100_drivers.json\", \"w\") as f:\n",
        "        f.write(json.dumps(top_k_drivers(df, 100).collect()))\n",
        "    # top worst drivers - variant 2 success\n",
        "    with open(\"data/worst_drivers.json\", \"w\") as f:\n",
        "        f.write(json.dumps(worst_drivers(df).collect()))\n",
        "    # dencent traffic by hour - variant 3 success\n",
        "    with open(\"data/densest_traffic_by_hour.json\", \"w\") as f:\n",
        "        f.write(json.dumps(densest_traffic_by_hour(df).collect()))\n",
        "    # top 50 clients - variant 4 success\n",
        "    with open(\"data/top_50_clients.json\", \"w\") as f:\n",
        "        f.write(json.dumps(top_k_clients(df, 50).collect()))\n",
        "    # top 50 night drivers - variant 6 - SUCCESS\n",
        "    with open(\"data/top_night_riders.json\", \"w\") as f:\n",
        "        f.write(json.dumps(top_night_riders(df, 50).collect()))\n",
        "    # most frequent category of good drivers - variant 7 SUCCESS\n",
        "    with open(\"data/top_praised_driver_category.json\", \"w\") as f:\n",
        "        f.write(json.dumps(top_driver_feedback_category(df).collect()))\n",
        "    # most frequent category of bad drivers - variant 8 SUCCESS\n",
        "    with open(\"data/top_complaint_driver_category.json\", \"w\") as f:\n",
        "        f.write(json.dumps(top_complaint_feedback_category(df).collect()))\n",
        "    # top 10 longest text comments SUCCESS\n",
        "    with open(\"data/top_10_longest_text_comments.json\", \"w\") as f:\n",
        "        f.write(json.dumps(top_10_longest_text_comment(df).collect()))"
      ],
      "metadata": {
        "id": "WI9VjrcRhKBh"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SHLQGnazhVk_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}